{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d498514-dc59-4cbe-b3ff-3a0cd760d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu as mw\n",
    "from scipy.stats import sem\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "import random\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import neo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from spike_filter import apply_intervals, get_intervals, get_spiketrains\n",
    "from utility import spiketrains_iterator, events_iterator\n",
    "import nexfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c97cfa-84f4-44ce-8531-6fd1a42b624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    \"patient\": [],\n",
    "    \"doc_name\": [],\n",
    "    \"data_name\": [],\n",
    "    \"interval_name\": [],\n",
    "    \"spt_list\": [],\n",
    "    \"isi_list\": [],\n",
    "}\n",
    "\n",
    "pat_path = r\"C:/Users/Nikita/Desktop/Лаба/Entropy/CD_GD_PD_2024/GPi/\"\n",
    "\n",
    "ISI_total = []\n",
    "ISI_change_total = []\n",
    "ISI_rel_change_total = []\n",
    "\n",
    "\n",
    "spike_c = 0\n",
    "\n",
    "# patients ID list\n",
    "pat_list = listdir(pat_path)\n",
    "# print(pat_list[0])\n",
    "r = nexfile.Reader()\n",
    "# patients processing\n",
    "for patient in pat_list:\n",
    "    print(\"Patient processing:\", patient)\n",
    "\n",
    "    temp_c = 0\n",
    "\n",
    "    start = time.time()\n",
    "    folder_path = pat_path + patient\n",
    "    onlyfiles = [f for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "    for f_name in onlyfiles:\n",
    "        if f_name.endswith(\".nex\"):  # More Pythonic way to check file extension\n",
    "            full_name = os.path.join(\n",
    "                folder_path, f_name\n",
    "            )  # Use os.path.join for better path handling\n",
    "            try:\n",
    "                file_data = r.ReadNexFile(filePath=full_name)\n",
    "            except Exception as e:  # Catch specific exceptions if possible\n",
    "                print(f\"Error reading {f_name}: {e}\")\n",
    "            spiketrains = list(get_spiketrains(file_data))\n",
    "            intervals = list(get_intervals(file_data))\n",
    "            for spiketrain_name, interval_name, spikes in apply_intervals(\n",
    "                spiketrains, intervals\n",
    "            ):\n",
    "                spikes = np.unique(spikes)\n",
    "                spikes = spikes - spikes[0]\n",
    "                isi = np.diff(spikes)\n",
    "                df_dict[\"patient\"].append(patient)\n",
    "                df_dict[\"doc_name\"].append(f_name)\n",
    "                df_dict[\"data_name\"].append(spiketrain_name)\n",
    "                df_dict[\"interval_name\"].append(interval_name)\n",
    "                df_dict[\"spt_list\"].append(list(spikes))\n",
    "                df_dict[\"isi_list\"].append(list(isi))\n",
    "                spike_c += 1\n",
    "                temp_c += 1\n",
    "    print(\"Patient's spiketrain count:\", temp_c)\n",
    "pd.DataFrame(df_dict).to_csv(\"spt_isi_GPi.csv\")\n",
    "print(\"Patient processing is over\")\n",
    "print(\"Total spiketrains count:\", spike_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103bbfe8-1a49-457d-9518-02df732a4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"spt_isi_GPi.csv\")\n",
    "\n",
    "\n",
    "def toFixed(numObj, digits=0):\n",
    "    return round(numObj, digits)\n",
    "\n",
    "\n",
    "def isitm(isi_list):\n",
    "    mean_isi = np.mean(isi_list)\n",
    "    return [1 if el >= mean_isi else 0 for el in isi_list]\n",
    "\n",
    "\n",
    "def isitisi(isi_list, dig=4):\n",
    "    isi_list_fixed = [toFixed(float(i), dig) for i in isi_list]\n",
    "    return [\n",
    "        3\n",
    "        if isi_list_fixed[i + 1] > isi_list_fixed[i]\n",
    "        else 2\n",
    "        if isi_list_fixed[i + 1] == isi_list_fixed[i]\n",
    "        else 1\n",
    "        for i in range(len(isi_list_fixed) - 1)\n",
    "    ]\n",
    "\n",
    "\n",
    "isitm_res = []\n",
    "isitisi_res = []\n",
    "\n",
    "list_to_drop = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    print(\"Нейрон №\", i + 1)\n",
    "    try:\n",
    "        # Convert isi_list from string to float list\n",
    "        isi_list = [float(j) for j in df[\"isi_list\"][i].strip(\"][\").split(\", \")]\n",
    "\n",
    "        isitm_res.append(isitm(isi_list))\n",
    "        isitisi_res.append(isitisi(isi_list))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        list_to_drop.append(i)\n",
    "\n",
    "# Drop rows with errors and reset index\n",
    "df.drop(index=list_to_drop, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add results to DataFrame\n",
    "df[\"isitm\"] = isitm_res\n",
    "df[\"isitisi\"] = isitisi_res\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"encoded_isi_GPi.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325256e-6973-4e4a-b033-24f3518b5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"encoded_isi_GPi.csv\", index_col=0\n",
    ")  # open file created with 2 CSV to ENCODED\n",
    "# Function for counting number of subsets in list\n",
    "\n",
    "\n",
    "def counter_1(base, find):\n",
    "    max_base = len(base) - len(find) + 1\n",
    "    result = 0\n",
    "    for i in range(max_base):\n",
    "        yes = True\n",
    "        for j in range(len(find)):\n",
    "            if base[i + j] != find[j]:\n",
    "                yes = False\n",
    "                break\n",
    "        if yes:\n",
    "            result += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def CAE_entropy(isi_change_list, word_len=2):\n",
    "    N = len(isi_change_list) - word_len + 1\n",
    "\n",
    "    rel_freq = {}  # Здесь хранится вероятность встретить каждый символ\n",
    "    counts_list = []\n",
    "    for i in range(0, N):\n",
    "        word = tuple(isi_change_list[i : i + word_len])\n",
    "        if word in rel_freq.keys():\n",
    "            pass\n",
    "        else:\n",
    "            counts = counter_1(isi_change_list, word)\n",
    "            counts_list.append(counts)\n",
    "            rel_freq[word] = counts / N\n",
    "    counts_list = np.array(counts_list)\n",
    "    counts_list = counts_list[counts_list > 0]\n",
    "    n = np.sum(counts_list)\n",
    "    p = counts_list / n\n",
    "\n",
    "    f1 = np.count_nonzero(counts_list == 1)\n",
    "    if f1 == n:\n",
    "        f1 = n - 1\n",
    "\n",
    "    C = 1 - f1 / n\n",
    "    pa = C * p\n",
    "    la = 1 - (1 - pa) ** n\n",
    "\n",
    "    return -np.sum(pa * np.log2(pa) / la), -np.sum(p * np.log2(p))  # CAE, ENT\n",
    "\n",
    "\n",
    "# Conditional entropy calculation\n",
    "\n",
    "\n",
    "def cond_ent(isi_list, samp_isi_list, word_len=2):\n",
    "    N = len(isi_list) - word_len + 1\n",
    "    M = len(samp_isi_list) - word_len + 1\n",
    "    if M != N:\n",
    "        return print(\"ERROR\")\n",
    "    r_prob = {}  # Здесь хранится вероятность встретить каждый символ\n",
    "    trial = {}\n",
    "    s_prob = {}\n",
    "    trial_list = []\n",
    "    for i in range(0, N):\n",
    "        word_r = tuple(isi_list[i : i + word_len])\n",
    "        word_s = tuple(samp_isi_list[i : i + word_len])\n",
    "        if word_r in r_prob.keys():\n",
    "            pass\n",
    "        else:\n",
    "            counts = counter_1(isi_list, word_r)\n",
    "            r_prob[word_r] = counts / N\n",
    "        if word_s in trial.keys():\n",
    "            trial[word_s].append(word_r)\n",
    "        else:\n",
    "            trial[word_s] = []\n",
    "            trial[word_s].append(word_r)\n",
    "            counts = counter_1(samp_isi_list, word_s)\n",
    "            s_prob[word_s] = counts / M\n",
    "    h_list = []\n",
    "    for word_s in s_prob.keys():\n",
    "        unique = set(trial[word_s])\n",
    "        for el in unique:\n",
    "            cond_prob = trial[word_s].count(el) / len(trial[word_s])\n",
    "            h_list.append(-s_prob[word_s] * cond_prob * np.log2(cond_prob))\n",
    "    return np.sum(h_list)\n",
    "\n",
    "\n",
    "params = [\"isitm\", \"isitisi\"]\n",
    "df = df[df[\"isitisi\"] != \"[]\"]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "ent_dict = {}\n",
    "low = 2\n",
    "top = 2\n",
    "for word_length in range(low, top + 1):\n",
    "    for param in params:\n",
    "        list_to_drop = []\n",
    "        print(param)\n",
    "        ent_param_name = \"ent_\" + param + \"_wl\" + str(word_length)\n",
    "        cond_ent_param_name = \"cond_ent_\" + param + \"_wl\" + str(word_length)\n",
    "        shuf_ent_param_name = \"shuf_ent_\" + param + \"_wl\" + str(word_length)\n",
    "\n",
    "        ent_dict[ent_param_name] = []\n",
    "        ent_dict[cond_ent_param_name] = []\n",
    "        ent_dict[shuf_ent_param_name] = []\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                data = df[param][i].strip(\"][\").split(\", \")\n",
    "                data = [float(j) for j in data]\n",
    "            except:\n",
    "                list_to_drop.append(i)\n",
    "                pass\n",
    "            res = CAE_entropy(data, word_len=word_length)\n",
    "\n",
    "            ent_dict[ent_param_name].append(res[1])\n",
    "\n",
    "            temp_res_cond_ent = []\n",
    "            temp_res_ent = []\n",
    "\n",
    "            for m in range(100):\n",
    "                samp_data = random.sample(data, k=len(data))\n",
    "\n",
    "                # calculating conditional entropy with shuffled signal\n",
    "                temp_res_cond = cond_ent(data, samp_data, word_len=word_length)\n",
    "                temp_res_cond_ent.append(temp_res_cond)\n",
    "\n",
    "                # calculation entropy of shuffled signal\n",
    "                temp_res = CAE_entropy(samp_data, word_len=word_length)\n",
    "                temp_res_ent.append(temp_res[1])\n",
    "\n",
    "            ent_dict[cond_ent_param_name].append(np.median(temp_res_cond_ent))\n",
    "            ent_dict[shuf_ent_param_name].append(np.median(temp_res_ent))\n",
    "    df = df.drop(columns=[\"spt_list\", \"isi_list\", \"isitm\", \"isitisi\"])\n",
    "    for key in ent_dict.keys():\n",
    "        df[key] = ent_dict[key]\n",
    "    df.to_csv(\"encoded_data_with_ents_GPi_wl\" + str(word_length) + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650756cd-8065-4591-9213-965579bb56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_excel(\"GPi.xls\")\n",
    "\n",
    "params = [\"isitm\", \"isitisi\"]\n",
    "\n",
    "files_list = []\n",
    "max_wl = 2\n",
    "for i in range(2, max_wl + 1):\n",
    "    files_list.append(\n",
    "        \"encoded_data_with_ents_GPi_wl\" + str(i) + \".csv\"\n",
    "    )  # open files created with 3 ENCODED ISI\n",
    "\n",
    "df = pd.read_csv(files_list[0])\n",
    "\n",
    "result = pd.merge(\n",
    "    df, raw_data, on=[\"patient\", \"doc_name\", \"data_name\", \"interval_name\"]\n",
    ")\n",
    "\n",
    "for i in range(2, 3):\n",
    "    for el in [\"isitisi\", \"isitm\"]:\n",
    "        MI = \"mut_inf_\" + el + \"_wl\" + str(i)  # mutual information I(X;Y)\n",
    "        X_ent = \"ent_\" + el + \"_wl\" + str(i)\n",
    "        cond_XY = \"cond_ent_\" + el + \"_wl\" + str(i)\n",
    "        result[MI] = result[X_ent] - result[cond_XY]\n",
    "\n",
    "\n",
    "result.to_csv(\"file_to_analyse_GPi.csv\")\n",
    "result.to_excel(\"file_to_analyse_GPi.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4e167-2f7d-49d8-bed8-95e462e8f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40bc5f3-786c-4d5a-923b-f006cfd045eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
